{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "# Open the NetCDF file\n",
    "file_path = 'data_25_degree.nc'\n",
    "fhadj = Dataset(file_path, mode='r')\n",
    "\n",
    "# Extract original data\n",
    "lon_sic = fhadj.variables['lon'][:120]  # Longitude\n",
    "lat_sic = fhadj.variables['lat'][:120]  # Latitude (original: -90 to -60)\n",
    "SIC = fhadj.variables['SIC'][:]  # Sea Ice Concentration (shape: time × lat × lon)\n",
    "time_SIC = fhadj.variables['time'][:]  # Time\n",
    "\n",
    "# Close the file\n",
    "fhadj.close()\n",
    "\n",
    "# Reverse latitude and SIC data\n",
    "lat_sic_reversed = lat_sic[::-1]  # Now goes from -60 to -90\n",
    "SIC_reversed = SIC[:, ::-1, :]  # Reverse along latitude (axis=1)\n",
    "\n",
    "SIC_reversed = np.where(SIC_reversed > 1, 0, SIC_reversed)  # Set negative values to zero\n",
    "lon_sic_reversed = lon_sic[::-1] \n",
    "\n",
    "\n",
    "\n",
    "# Verify\n",
    "print(\"Original latitude range:\", lat_sic[0], \"to\", lat_sic[-1])\n",
    "print(\"Reversed latitude range:\", lat_sic_reversed[0], \"to\", lat_sic_reversed[-1])\n",
    "print(\"SIC shape before reversal:\", SIC.shape)\n",
    "print(\"SIC shape after reversal:\", SIC_reversed.shape)\n",
    "print(\"Min SIC:\", np.min(SIC_reversed))\n",
    "print(\"Max SIC:\", np.max(SIC_reversed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming time_SIC is in YYYYMMDD format (e.g., 20190101)\n",
    "time_SIC_dates = time_SIC.astype(int)  # Ensure it's integer if not already\n",
    "\n",
    "# Define the start and end dates in YYYYMMDD format\n",
    "start_date = 20191101\n",
    "end_date = 20200301\n",
    "\n",
    "# Find indices where time_SIC is within the desired range\n",
    "mask = (time_SIC_dates >= start_date) & (time_SIC_dates < end_date)\n",
    "time_indices = np.where(mask)[0]\n",
    "\n",
    "print(f\"Found {len(time_indices)} time steps between {start_date} and {end_date}\")\n",
    "\n",
    "# Extract the SIC data for the selected time range\n",
    "selected_SIC = SIC_reversed[time_indices, -120:, :]  # Shape: (time, lat, lon)\n",
    "\n",
    "# Verify the extracted data\n",
    "print(\"\\n--- Extracted Data Summary ---\")\n",
    "print(f\"Shape of selected_SIC: {selected_SIC.shape}\")\n",
    "print(f\"First date in selection: {time_SIC_dates[time_indices[0]]}\")\n",
    "print(f\"Last date in selection: {time_SIC_dates[time_indices[-1]]}\")\n",
    "print(f\"Min SIC in selection: {np.min(selected_SIC)}\")\n",
    "print(f\"Max SIC in selection: {np.max(selected_SIC)}\")\n",
    "\n",
    "# Close the NetCDF file\n",
    "\n",
    "\n",
    "# November 2019: 30 days → indices 0 to 29\n",
    "nov_indices = list(range(0, 30))\n",
    "\n",
    "# December 2019: 31 days → indices 30 to 60\n",
    "dec_indices = list(range(30, 61))\n",
    "\n",
    "# January 2020: 31 days → indices 61 to 91\n",
    "jan_indices = list(range(61, 92))\n",
    "\n",
    "# February 2020: 29 days (leap year) → indices 92 to 120\n",
    "feb_indices = list(range(92, 121))\n",
    "\n",
    "\n",
    "print(nov_indices)\n",
    "print(dec_indices)\n",
    "print(jan_indices)\n",
    "print(feb_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize an empty list to store daily differences\n",
    "SIC_daily_diff_list = []\n",
    "\n",
    "# Loop through selected_SIC and compute differences\n",
    "for i in range(len(selected_SIC) - 1):  # Stop at second-to-last day\n",
    "    daily_diff = selected_SIC[i+1] - selected_SIC[i]  # day[i+1] - day[i]\n",
    "    SIC_daily_diff_list.append(625*daily_diff)\n",
    "\n",
    "# Convert the list to a NumPy array (optional, but useful for further analysis)\n",
    "SIC_daily_diff_array = np.array(SIC_daily_diff_list)\n",
    "\n",
    "# Verify results\n",
    "print(\"--- Daily Differences (Loop Method) ---\")\n",
    "print(f\"Number of computed differences: {len(SIC_daily_diff_list)}\")\n",
    "print(f\"Shape of first difference: {SIC_daily_diff_list[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "# Open the NetCDF files\n",
    "filtered_file_1 = '2019_4_month.nc'\n",
    "fhadj_1 = Dataset(filtered_file_1, mode='r')\n",
    "\n",
    "#61 is staring of November\n",
    "\n",
    "ice_sheets_1 = fhadj_1.variables['sd'][61:]\n",
    "ice_sheets_1_lat = fhadj_1.variables['latitude'][-120:]\n",
    "ice_sheets_1_lon_before_t = fhadj_1.variables['longitude'][:]\n",
    "\n",
    "filtered_file_2 = '2020_jan_feb.nc'\n",
    "fhadj_2 = Dataset(filtered_file_2, mode='r')\n",
    "ice_sheets_2 = fhadj_2.variables['sd'][:]\n",
    "\n",
    "# Close the NetCDF files\n",
    "fhadj_1.close()\n",
    "fhadj_2.close()\n",
    "\n",
    "# Concatenate ice_sheets_1 and ice_sheets_2 along the time axis\n",
    "combined_ice_sheets_before_t = np.concatenate((ice_sheets_1, ice_sheets_2), axis=0)\n",
    "\n",
    "# Convert longitude from 0–360 to -180–180\n",
    "ice_sheets_1_lon_after_180 = np.where(\n",
    "    ice_sheets_1_lon_before_t > 180,\n",
    "    ice_sheets_1_lon_before_t - 360,\n",
    "    ice_sheets_1_lon_before_t\n",
    ")\n",
    "\n",
    "# Find where the longitude wraps and roll data to make it continuous\n",
    "split_idx = np.argmax(ice_sheets_1_lon_after_180 < 0)\n",
    "combined_ice_sheets = np.roll(combined_ice_sheets_before_t, -split_idx, axis=2)\n",
    "ice_sheets_1_lon = np.roll(ice_sheets_1_lon_after_180, -split_idx)\n",
    "\n",
    "# Compute daily differences\n",
    "daily_diff = 625*np.diff(combined_ice_sheets, axis=0)\n",
    "\n",
    "# Slice to keep only latitudes 0:120 (and all longitudes)\n",
    "daily_diff_sliced = daily_diff[:, -120:, :]\n",
    "\n",
    "print(f\"Shape of sliced daily differences: {daily_diff_sliced.shape}\")\n",
    "print(f\"Longitude range after conversion: {ice_sheets_1_lon.min()}, {ice_sheets_1_lon.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'selected_SIC' and 'combined_ice_sheets' are already defined\n",
    "# Select the first time step for demonstration (index 0)\n",
    "sic_for_graph_building = SIC_daily_diff_array[:, :, 200:600]  # SIC data for the first time step\n",
    "ice_sheets_for_graph_building = daily_diff_sliced[:, -120:, 200:600]\n",
    "\n",
    "sic_for_graph_building_plot = selected_SIC[:, :, 200:600]  # SIC data for the first time step\n",
    "ice_sheets_for_graph_building_plot = combined_ice_sheets[:, -120:, 200:600]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('lat_sic')\n",
    "\n",
    "lat_sic_reversed = lat_sic_reversed[:,0]\n",
    "\n",
    "print(lat_sic_reversed.shape)\n",
    "print('lon_sic')\n",
    "print(lat_sic_reversed[0])\n",
    "\n",
    "lon_sic_reversed=lon_sic_reversed[0,200:600]\n",
    "\n",
    "print(lon_sic_reversed.shape)\n",
    "\n",
    "print(lon_sic_reversed[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('lat_ice')\n",
    "print(ice_sheets_1_lat.shape)\n",
    "print(ice_sheets_1_lat[0])\n",
    "\n",
    "ice_sheets_1_lon = ice_sheets_1_lon[200:600]\n",
    "print('lon_ice')\n",
    "print(ice_sheets_1_lon.shape)\n",
    "print(ice_sheets_1_lon[0])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract first time step data\n",
    "sic_data = sic_for_graph_building_plot[0]            # shape: (lat, lon)\n",
    "ice_data = ice_sheets_for_graph_building_plot[0]    # shape: (lat, lon)\n",
    "\n",
    "# Latitude and longitude ranges (optional for extent)\n",
    "lat_range = ice_sheets_1_lat                     # shape: (120,)\n",
    "lon_range = ice_sheets_1_lon_after_180           # shape: (421,)\n",
    "\n",
    "# Create side-by-side subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot Sea Ice Concentration\n",
    "cax1 = axes[0].imshow(sic_data, cmap='Blues', interpolation='none', alpha=0.9)\n",
    "axes[0].set_title('Sea Ice Concentration')\n",
    "axes[0].set_xlabel('Longitude')\n",
    "axes[0].set_ylabel('Latitude')\n",
    "fig.colorbar(cax1, ax=axes[0], label='SIC')\n",
    "\n",
    "# Plot Ice Sheets\n",
    "cax2 = axes[1].imshow(ice_data, cmap='Reds', interpolation='none', alpha=0.9)\n",
    "axes[1].set_title('Ice Sheets')\n",
    "axes[1].set_xlabel('Longitude')\n",
    "axes[1].set_ylabel('Latitude')\n",
    "fig.colorbar(cax2, ax=axes[1], label='Ice Thickness')\n",
    "\n",
    "# Adjust layout and show\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import Delaunay\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import Delaunay\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from collections import deque\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import warnings\n",
    "from scipy.spatial import cKDTree\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "from scipy.spatial import Delaunay\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "\n",
    "thres_defs = [\n",
    "    \"median–q3 (SIC), median–q3 (ICE)\",\n",
    "    \"median–q3 (SIC), q3–ub (ICE)\",\n",
    "    \"median–q3 (SIC), > ub (ICE)\",\n",
    "    \"q3–ub (SIC), median–q3 (ICE)\",\n",
    "    \"q3–ub (SIC), q3–ub (ICE)\",\n",
    "    \"q3–ub (SIC), > ub (ICE)\",\n",
    "    \"> ub (SIC), median–q3 (ICE)\",\n",
    "    \"> ub (SIC), q3–ub (ICE)\",\n",
    "    \"> ub (SIC), > ub (ICE)\"\n",
    "]\n",
    "\n",
    "thres_def = thres_defs[0]\n",
    "\n",
    "\n",
    "def is_in_range(val, thresholds, range_type):\n",
    "    if range_type == 'r1':\n",
    "        return thresholds['median'] <= val <= thresholds['q3']\n",
    "    elif range_type == 'r2':\n",
    "        return thresholds['q3'] < val <= thresholds['ub']\n",
    "    elif range_type == 'r3':\n",
    "        return val > thresholds['ub']\n",
    "\n",
    "\n",
    "def create_graph(sic_changes, ice_changes, sic_thresholds, ice_thresholds):\n",
    "    try:\n",
    "        ocean_points = np.argwhere(sic_changes >0)\n",
    "\n",
    "        land_points = np.argwhere(ice_changes >0)\n",
    "\n",
    "\n",
    "        # print(ocean_points.shape)\n",
    "        # print(land_points.shape)\n",
    "        # print('after removing cimmmn points')\n",
    "        # Convert coordinate arrays to sets of tuples for comparison\n",
    "        ocean_tuples = {tuple(point) for point in ocean_points}\n",
    "        land_tuples = {tuple(point) for point in land_points}\n",
    "\n",
    "        # Find overlapping coordinates\n",
    "        common_points = ocean_tuples & land_tuples\n",
    "\n",
    "        if common_points:\n",
    "            #print(f\"Warning: {len(common_points)} locations have both sea ice and ice sheets:\")\n",
    "            # for point in common_points:\n",
    "            #     print(f\"  - Row {point[0]}, Column {point[1]}\")\n",
    "                \n",
    "            # Treat common points as sea ice\n",
    "            # Remove common points from the sea ice set\n",
    "            #ocean_tuples -= common_points\n",
    "            land_tuples -= common_points\n",
    "            \n",
    "\n",
    "            ##print(\"Common points are now treated as sea ice, and removed from the land ice set.\")\n",
    "\n",
    "        # else:\n",
    "        #     print(\"No overlapping locations found - sea ice and ice sheets are properly separated\")\n",
    "\n",
    "        # Now continue with your logic\n",
    "            \n",
    "\n",
    "        \n",
    "        ocean_points = np.array(list(ocean_tuples))\n",
    "        land_points = np.array(list(land_tuples))\n",
    "    \n",
    "\n",
    "        # print(ocean_points.shape)\n",
    "        # print(land_points.shape)\n",
    "\n",
    "        # print('next')\n",
    "\n",
    "        \n",
    "        if len(ocean_points) + len(land_points) < 4:\n",
    "            #print(\"Not enough points for triangulation\")\n",
    "            return None\n",
    "\n",
    "\n",
    "        all_points = np.vstack([ocean_points, land_points])\n",
    "        labels = ['O'] * len(ocean_points) + ['L'] * len(land_points)\n",
    "\n",
    "        tri = Delaunay(all_points)\n",
    "\n",
    "        point_labels = {\n",
    "            tuple(p): f\"{label}({p[0]},{p[1]})\"\n",
    "            for p, label in zip(all_points, labels)\n",
    "        }\n",
    "\n",
    "        G = nx.Graph()\n",
    "        G.graph['ocean_points'] = ocean_points\n",
    "        G.graph['land_points'] = land_points\n",
    "\n",
    "        #Add all nodes with correct change values\n",
    "        for point, label in zip(all_points, labels):\n",
    "            row, col = point\n",
    "            node_name = point_labels[tuple(point)]\n",
    "            \n",
    "            if label == 'O':\n",
    "                change_val = sic_changes[row, col]\n",
    "                G.add_node(node_name, label='O', sic_data=change_val)\n",
    "            else:\n",
    "                change_val = ice_changes[row, col]\n",
    "                G.add_node(node_name, label='L', ice_data=change_val)\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "        for simplex in tri.simplices:\n",
    "            for i in range(3):\n",
    "                for j in range(i + 1, 3):\n",
    "                    p1 = tuple(all_points[simplex[i]])\n",
    "                    p2 = tuple(all_points[simplex[j]])\n",
    "                    node1, node2 = point_labels[p1], point_labels[p2]\n",
    "\n",
    "                    # Compute Euclidean distance\n",
    "                    dist = np.linalg.norm(np.array(p1) - np.array(p2))\n",
    "                    if dist > 11:\n",
    "                        continue  # skip if distance is greater than 8\n",
    "\n",
    "                    # Get changes for the nodes\n",
    "                    def get_change(point, label):\n",
    "                        row, col = point\n",
    "                        return sic_changes[row, col] if label == 'O' else ice_changes[row, col]\n",
    "\n",
    "                    label1, label2 = node1[0], node2[0]\n",
    "                    change1 = get_change(p1, label1)\n",
    "                    change2 = get_change(p2, label2)\n",
    "\n",
    "                    # Apply thresholds\n",
    "                    # Activate only this block (SIC r1, ICE r2)\n",
    "                    if label1 == 'O' and not is_in_range(change1, sic_thresholds, 'r1'):\n",
    "                        continue\n",
    "                    if label1 == 'L' and not is_in_range(change1, ice_thresholds, 'r1'):\n",
    "                        continue\n",
    "                    if label2 == 'O' and not is_in_range(change2, sic_thresholds, 'r1'):\n",
    "                        continue\n",
    "                    if label2 == 'L' and not is_in_range(change2, ice_thresholds, 'r1'):\n",
    "                        continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    weight = 1 if (change1 * change2 > 0) else -1\n",
    "\n",
    "                    if (label1 == 'O' and label2 == 'L') or (label1 == 'L' and label2 == 'O'):\n",
    "                        G.add_edge(\n",
    "                            node1, node2,\n",
    "                            weight=weight,\n",
    "                            distance=dist,\n",
    "                            sic_data=change1 if label1 == 'O' else change2,\n",
    "                            ice_data=change2 if label2 == 'L' else change1\n",
    "                        )\n",
    "                    else:\n",
    "                        G.add_edge(\n",
    "                            node1, node2,\n",
    "                            weight=weight,\n",
    "                            distance=dist\n",
    "                        )\n",
    "\n",
    "        return G\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Delaunay-based graph creation failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def find_positive_paths(all_G, sic_thresholds, ice_thresholds, time_range, max_length, M=500, n_workers=4):\n",
    "    \"\"\"Complete pipeline for positive correlation analysis\"\"\"\n",
    "    # 1. Filter valid graphs\n",
    "    valid_graphs = [G for G in all_G if G is not None]\n",
    "    if not valid_graphs:\n",
    "        print(\"No valid graphs found!\")\n",
    "        return []\n",
    "\n",
    "    # 2. BFS Pathfinding\n",
    "    print(\"Finding positive correlation paths...\")\n",
    "    all_positive_paths = []\n",
    "    for G in tqdm(valid_graphs):\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        paths = bfs_paths_icesheet15(G, max_length)\n",
    "        all_positive_paths.append(paths)\n",
    "\n",
    "\n",
    "        # we need to uncomment to get bfs_paths_seaice5_icesheet10\n",
    "\n",
    "\n",
    "        # paths = bfs_paths_icesheet15(G, max_length)\n",
    "        # all_positive_paths.append(paths)\n",
    "\n",
    "        #print(paths)\n",
    "    \n",
    "    # 3. Monte Carlo Testing\n",
    "    print(\"Running significance tests...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    significant_paths = []\n",
    "    for G, paths in zip(valid_graphs, all_positive_paths):\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        if paths:\n",
    "            (max_pos, p_pos), _ = parallel_monte_carlo(G, paths, sic_thresholds, ice_thresholds, M, n_workers)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            #print(max_pos,p_pos)\n",
    "\n",
    "\n",
    "\n",
    "            sig_paths = [path for path in paths if is_significant_path(G, path, p_pos)]\n",
    "            significant_paths.append(sig_paths)\n",
    "            print('G',G)\n",
    "            print('paths',sig_paths)\n",
    "        else:\n",
    "            significant_paths.append([])\n",
    "    \n",
    "    # 4. Visualization\n",
    "    if any(sig_paths for sig_paths in significant_paths):\n",
    "        #plot_positive_paths(valid_graphs, significant_paths, time_range)\n",
    "        print('we ve significant paths')\n",
    "    else:\n",
    "        print(\"No significant paths found for visualization\")\n",
    "    \n",
    "    return significant_paths\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "#it will start from ice sheet nodes and end with ice sheet nodes\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def bfs_paths_icesheet15(G, max_length=15):\n",
    "    \"\"\"\n",
    "    Find all simple positive-edge paths of EXACT length 15:\n",
    "      - Start node is ice-sheet: L(...)\n",
    "      - End node is ice-sheet: L(...)\n",
    "      - All nodes are ice-sheet nodes\n",
    "      - Total path length = 15\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : networkx.Graph or DiGraph\n",
    "        Graph with edge attribute 'weight'.\n",
    "    max_length : int\n",
    "        Exact path length (default = 15)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[list[str]]\n",
    "        All valid ice-sheet-only paths.\n",
    "    \"\"\"\n",
    "\n",
    "    is_ice_sheet = lambda n: isinstance(n, str) and n.startswith(\"L(\")\n",
    "\n",
    "    ice_sheet_nodes = [n for n in G.nodes if is_ice_sheet(n)]\n",
    "    valid_paths = []\n",
    "\n",
    "    for start in ice_sheet_nodes:\n",
    "        queue = deque([(start, [start])])\n",
    "\n",
    "        while queue:\n",
    "            node, path = queue.popleft()\n",
    "            k = len(path)\n",
    "\n",
    "            # If exact length reached, record and stop expanding\n",
    "            if k == max_length:\n",
    "                valid_paths.append(path)\n",
    "                continue\n",
    "\n",
    "            # Do not exceed length\n",
    "            if k > max_length:\n",
    "                continue\n",
    "\n",
    "            for nbr in G.neighbors(node):\n",
    "                # No cycles\n",
    "                if nbr in path:\n",
    "                    continue\n",
    "\n",
    "                # Positive-edge constraint\n",
    "                if G[node][nbr].get(\"weight\", 0) <= 0:\n",
    "                    continue\n",
    "\n",
    "                # Ice-sheet only constraint\n",
    "                if not is_ice_sheet(nbr):\n",
    "                    continue\n",
    "\n",
    "                queue.append((nbr, path + [nbr]))\n",
    "\n",
    "    return valid_paths\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "## it will consist of first 5 sea ice nodes and last 10 ice sheet nodes\n",
    "\n",
    "def bfs_paths_seaice5_icesheet10(G,max_length=15):\n",
    "    \"\"\"\n",
    "    Find all simple positive-edge paths of EXACT length 15:\n",
    "      - First 5 nodes are sea-ice nodes:  O(...)\n",
    "      - Last 10 nodes are ice-sheet nodes: L(...)\n",
    "    Total length = 5 + 10 = 15\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : networkx.Graph or DiGraph\n",
    "        Graph with edge attribute 'weight'.\n",
    "    Returns\n",
    "    -------\n",
    "    list[list[str]]\n",
    "        All valid paths (each a list of node labels).\n",
    "    \"\"\"\n",
    "\n",
    "    is_sea_ice   = lambda n: isinstance(n, str) and n.startswith(\"O(\")\n",
    "    is_ice_sheet = lambda n: isinstance(n, str) and n.startswith(\"L(\")\n",
    "\n",
    "    # We enforce exact length 15\n",
    "    MAX_LEN = 15\n",
    "    SEA_LEN = 5\n",
    "    ICE_LEN = 10\n",
    "\n",
    "    sea_ice_nodes = [n for n in G.nodes if is_sea_ice(n)]\n",
    "    valid_paths = []\n",
    "\n",
    "    for start in sea_ice_nodes:\n",
    "        # start must be sea-ice\n",
    "        queue = deque([(start, [start])])\n",
    "\n",
    "        while queue:\n",
    "            node, path = queue.popleft()\n",
    "            k = len(path)\n",
    "\n",
    "            # If we reached length 15, validate the whole pattern\n",
    "            if k == MAX_LEN:\n",
    "                if all(is_sea_ice(x) for x in path[:SEA_LEN]) and all(is_ice_sheet(x) for x in path[SEA_LEN:]):\n",
    "                    valid_paths.append(path)\n",
    "                continue\n",
    "\n",
    "            # Decide what type the NEXT node must be\n",
    "            # positions are 1-indexed in description, but here use k (current length)\n",
    "            # If k < 5: we are still building sea-ice part => next must be sea-ice\n",
    "            # If k >= 5: now in ice-sheet part => next must be ice-sheet\n",
    "            require_next_sea = (k < SEA_LEN)\n",
    "\n",
    "            for nbr in G.neighbors(node):\n",
    "                # simple path constraint\n",
    "                if nbr in path:\n",
    "                    continue\n",
    "\n",
    "                # positive edge constraint\n",
    "                if G[node][nbr].get(\"weight\", 0) <= 0:\n",
    "                    continue\n",
    "\n",
    "                # enforce node-type constraint for next node\n",
    "                if require_next_sea:\n",
    "                    if not is_sea_ice(nbr):\n",
    "                        continue\n",
    "                else:\n",
    "                    if not is_ice_sheet(nbr):\n",
    "                        continue\n",
    "\n",
    "                queue.append((nbr, path + [nbr]))\n",
    "\n",
    "    return valid_paths\n",
    "\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "### It will start from sea ice nodes and end with three ice sheet nodes\n",
    "\n",
    "def bfs_positive_paths(G, max_length):\n",
    "    \"\"\"\n",
    "    Find all simple positive-edge paths that start on sea-ice nodes (O…)\n",
    "    and end with THREE consecutive ice-sheet nodes (L…L…L…).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : networkx.Graph\n",
    "        Graph with 'weight' on edges.\n",
    "    max_length : int\n",
    "        Hard cap on the total number of nodes in any returned path.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[list[str]]\n",
    "        Paths that satisfy the criteria.\n",
    "    \"\"\"\n",
    "    sea_ice_nodes   = [n for n in G if n.startswith('O(')]\n",
    "    is_ice_sheet    = lambda n: n.startswith('L(')\n",
    "    paths = []\n",
    "\n",
    "    for start in sea_ice_nodes:\n",
    "        queue   = deque([(start, [start])])\n",
    "        visited = {start}\n",
    "\n",
    "        while queue:\n",
    "            node, path = queue.popleft()\n",
    "\n",
    "            # ── Check end condition ──────────────────────────────────────────\n",
    "            if (len(path) >= 4                                 # need room for O… + L,L,L\n",
    "                and len(path) <= max_length\n",
    "                and all(is_ice_sheet(n) for n in path[-3:])):  # last 3 are ice sheets\n",
    "                paths.append(path)\n",
    "                # do NOT continue exploring from here—comment out next line\n",
    "                continue\n",
    "\n",
    "            # ── Depth limit ─────────────────────────────────────────────────\n",
    "            if len(path) >= max_length:\n",
    "                continue\n",
    "\n",
    "            # ── Positive-edge expansion ─────────────────────────────────────\n",
    "            for nbr in G.neighbors(node):\n",
    "                if nbr not in visited and G[node][nbr].get('weight', 0) > 0:\n",
    "                    visited.add(nbr)\n",
    "                    queue.append((nbr, path + [nbr]))\n",
    "\n",
    "    return paths\n",
    "\n",
    "\n",
    "\n",
    "# def bfs_positive_paths(G, max_length):\n",
    "#     \"\"\"BFS for paths between sea ice (O) and ice sheets (L) with positive edges\"\"\"\n",
    "#     # Get all sea ice (O) and ice sheet (L) nodes\n",
    "#     sea_ice_nodes = [n for n in G.nodes if n.startswith('O(')]\n",
    "#     ice_sheet_nodes = [n for n in G.nodes if n.startswith('L(')]\n",
    "#     ice_sheet_set = set(ice_sheet_nodes)\n",
    "    \n",
    "#     paths = []\n",
    "#     for start in sea_ice_nodes:\n",
    "#         queue = deque([(start, [start])])\n",
    "#         visited = set([start])\n",
    "        \n",
    "#         while queue:\n",
    "#             node, path = queue.popleft()\n",
    "            \n",
    "#             # Found path to ice sheet\n",
    "#             if node in ice_sheet_set and len(path) > 1:\n",
    "#                 paths.append(path)\n",
    "#                 continue\n",
    "                \n",
    "#             if len(path) >= max_length:\n",
    "#                 continue\n",
    "                \n",
    "#             # Explore positive edges only\n",
    "#             for neighbor in G.neighbors(node):\n",
    "#                 if neighbor not in visited and G[node][neighbor]['weight'] > 0:\n",
    "#                     visited.add(neighbor)\n",
    "#                     queue.append((neighbor, path + [neighbor]))\n",
    "    \n",
    "#     return paths\n",
    "\n",
    "def is_significant_path(G, path, p_pos, alpha=0.05):\n",
    "    \"\"\"Check if path has at least one significant positive edge\"\"\"\n",
    "    return any(p_pos < alpha \n",
    "              for u,v in zip(path[:-1], path[1:]) \n",
    "              if G.has_edge(u,v))\n",
    "\n",
    "def parallel_monte_carlo(G, paths, sic_thresholds, ice_thresholds, M=500, n_workers=4):\n",
    "    \"\"\"Parallel Monte Carlo simulation for significance testing\"\"\"\n",
    "    # Get original data from graph attributes\n",
    "    ocean_points = G.graph['ocean_points']\n",
    "    land_points = G.graph['land_points']\n",
    "    sic_data = np.zeros((np.max(ocean_points[:,0])+1, np.max(ocean_points[:,1])+1))\n",
    "    ice_data = np.zeros((np.max(land_points[:,0])+1, np.max(land_points[:,1])+1))\n",
    "    \n",
    "    # Reconstruct original data arrays from edges\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        if u.startswith('O(') and v.startswith('L('):\n",
    "            o_row, o_col = map(int, u[2:-1].split(','))\n",
    "            l_row, l_col = map(int, v[2:-1].split(','))\n",
    "            sic_data[o_row, o_col] = data['sic_data']\n",
    "            ice_data[l_row, l_col] = data['ice_data']\n",
    "    \n",
    "    # Calculate observed scores\n",
    "    observed_pos = []\n",
    "    observed_neg = []\n",
    "    for path in paths:\n",
    "        max_pos = 0\n",
    "        max_neg = 0\n",
    "        for i in range(len(path)-1):\n",
    "            u, v = path[i], path[i+1]\n",
    "            if G.has_edge(u, v):\n",
    "                weight = G[u][v]['weight']\n",
    "                if weight > max_pos:\n",
    "                    max_pos = weight\n",
    "                elif weight < -max_neg:\n",
    "                    max_neg = -weight\n",
    "        observed_pos.append(max_pos)\n",
    "        observed_neg.append(max_neg)\n",
    "    \n",
    "    max_observed_pos = max(observed_pos) if observed_pos else 0\n",
    "    max_observed_neg = max(observed_neg) if observed_neg else 0\n",
    "    \n",
    "    # Parallel processing\n",
    "    def process_chunk(chunk_size):\n",
    "        null_pos = []\n",
    "        null_neg = []\n",
    "        for _ in range(chunk_size):\n",
    "            # Shuffle while preserving ocean/land structure\n",
    "            shuffled_ice = np.random.permutation(ice_data.reshape(-1)).reshape(ice_data.shape)\n",
    "            G_shuffled = create_graph(sic_data, shuffled_ice, sic_thresholds, ice_thresholds)\n",
    "            \n",
    "            if G_shuffled:\n",
    "                for path in paths:\n",
    "                    max_p = 0\n",
    "                    max_n = 0\n",
    "                    for i in range(len(path)-1):\n",
    "                        u, v = path[i], path[i+1]\n",
    "                        if G_shuffled.has_edge(u, v):\n",
    "                            weight = G_shuffled[u][v]['weight']\n",
    "                            if weight > max_p:\n",
    "                                max_p = weight\n",
    "                            elif weight < -max_n:\n",
    "                                max_n = -weight\n",
    "                    null_pos.append(max_p)\n",
    "                    null_neg.append(max_n)\n",
    "        return null_pos, null_neg\n",
    "    \n",
    "    # Run simulations\n",
    "    chunk_size = max(1, M // n_workers)\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        futures = [executor.submit(process_chunk, chunk_size) for _ in range(n_workers)]\n",
    "        results = [f.result() for f in futures]\n",
    "    \n",
    "    # Combine results\n",
    "    null_pos = np.concatenate([r[0] for r in results])\n",
    "    null_neg = np.concatenate([r[1] for r in results])\n",
    "    \n",
    "    # Calculate p-values\n",
    "    p_pos = (np.sum(null_pos >= max_observed_pos) + 1) / (len(null_pos) + 1)\n",
    "    p_neg = (np.sum(null_neg >= max_observed_neg) + 1) / (len(null_neg) + 1)\n",
    "    \n",
    "    return (max_observed_pos, p_pos), (max_observed_neg, p_neg)\n",
    "\n",
    "def plot_positive_paths(all_G, significant_paths, time_range):\n",
    "    \"\"\"Visualize significant positive paths\"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Combine all graphs for layout\n",
    "    combined_G = nx.compose_all(all_G)\n",
    "    pos = {node: (int(node.split(',')[0][2:]), int(node.split(',')[1][:-1])) \n",
    "           for node in combined_G.nodes()}\n",
    "    \n",
    "    # Draw all nodes\n",
    "    nx.draw_networkx_nodes(combined_G, pos, node_size=3, node_color='gray', alpha=0.1)\n",
    "    \n",
    "    # Draw significant paths\n",
    "    for G, paths in zip(all_G, significant_paths):\n",
    "        for path in paths:\n",
    "            edges = [(u,v) for u,v in zip(path[:-1], path[1:]) \n",
    "                    if G.has_edge(u,v)]\n",
    "            nx.draw_networkx_edges(G, pos, edgelist=edges, \n",
    "                                 edge_color='red', width=2.5, alpha=0.7)\n",
    "    \n",
    "    # Highlight endpoints\n",
    "    starts = {path[0] for paths in significant_paths for path in paths}\n",
    "    ends = {path[-1] for paths in significant_paths for path in paths}\n",
    "    \n",
    "    nx.draw_networkx_nodes(combined_G, pos, nodelist=starts,\n",
    "                         node_color='lime', node_size=80, alpha=0.8)\n",
    "    nx.draw_networkx_nodes(combined_G, pos, nodelist=ends,\n",
    "                         node_color='magenta', node_size=80, alpha=0.8)\n",
    "    \n",
    "    plt.title(f\"Significant Positive Paths ({time_range[0]}-{time_range[1]})\\n\"\n",
    "              f\"Total: {sum(len(p) for p in significant_paths)} paths\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"positive_paths_{time_range[0]}_{time_range[1]}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Your existing parallel_monte_carlo can remain unchanged\n",
    "\n",
    "def get_thresholds(data):\n",
    "    \"\"\"Calculate median, Q3, and upper bound (UB) thresholds\"\"\"\n",
    "    flat_data = data[data != 0]  # Remove zeros\n",
    "\n",
    "    if len(flat_data) == 0:\n",
    "        return {'median': 0, 'q3': 0, 'ub': 0}\n",
    "\n",
    "    median = np.median(flat_data)\n",
    "    q1 = np.percentile(flat_data, 25)\n",
    "    q3 = np.percentile(flat_data, 75)\n",
    "    iqr = q3 - q1\n",
    "    ub = q3 + 1.5 * iqr  # Upper bound\n",
    "\n",
    "    return {\n",
    "        'median': median,\n",
    "        'q3': q3,\n",
    "        'ub': ub\n",
    "    }\n",
    "\n",
    "# 6. Main Analysis Function\n",
    "def analyze_ice_relationships(sic_data, ice_data, time_idx, sic_thresholds, ice_thresholds):\n",
    "\n",
    "   \n",
    "   \n",
    "    G = create_graph(sic_data, ice_data, sic_thresholds, ice_thresholds)\n",
    "\n",
    "\n",
    "    print('time_index',time_idx)\n",
    "\n",
    "    print(f\"Found {len(G.edges)} significant ocean-land connections\")\n",
    "\n",
    "    \n",
    "    if G is not None:\n",
    "        print(f\"Found {len(G.edges)} significant ocean-land connections\")\n",
    "        #for edge in G.edges(data=True):\n",
    "            # print(f\"{edge[0]} ↔ {edge[1]} | \"\n",
    "            #     f\"Weight: {edge[2]['weight']} | \"\n",
    "            #     f\"Distance: {edge[2]['distance']:.1f} grid cells\")\n",
    "    else:\n",
    "        print(f\"Graph could not be created for time index {time_idx}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return G\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "# 7. Data Preprocessing\n",
    "def preprocess_data(sic_data, ice_data, time_idx):\n",
    "    \"\"\"Prepare data for analysis\"\"\"\n",
    "    sic_processed = np.where(sic_data[time_idx] < 0, -sic_data[time_idx], 0)\n",
    "    ice_processed = np.where(ice_data[time_idx] < 0, -ice_data[time_idx], 0)\n",
    "    return sic_processed, ice_processed\n",
    "\n",
    "\n",
    "def count_median_to_q3(data, thresholds):\n",
    "    median = thresholds['median']\n",
    "    q3 = thresholds['q3']\n",
    "    return ((data > median) & (data <= q3)).sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    " # Adjust as needed\n",
    "all_sic = []\n",
    "all_ice = []\n",
    "\n",
    "\n",
    "sic_for_saving = []\n",
    "ice_sheets_for_saving = []\n",
    "\n",
    "for time_idx in range(0,120):\n",
    "    # Preprocess data\n",
    "    sic_processed, ice_processed = preprocess_data(sic_for_graph_building, ice_sheets_for_graph_building, time_idx)\n",
    "    \n",
    "    # Stack data for global threshold calculation\n",
    "\n",
    "    sic_for_saving.append(sic_processed)\n",
    "    ice_sheets_for_saving.append(ice_processed)\n",
    "\n",
    "    all_sic.append(sic_processed.flatten())\n",
    "    all_ice.append(ice_processed.flatten())\n",
    "\n",
    "# Concatenate all data across time range\n",
    "stacked_sic = np.concatenate(all_sic)\n",
    "stacked_ice = np.concatenate(all_ice)\n",
    "\n",
    "# Compute thresholds\n",
    "sic_thresh = get_thresholds(stacked_sic)\n",
    "ice_thresh = get_thresholds(stacked_ice)\n",
    "\n",
    "print(\"\\nGlobal Thresholds from Time 0 to 121:\")\n",
    "print(f\"SIC: median={sic_thresh['median']:.2f}, q3={sic_thresh['q3']:.2f}, ub={sic_thresh['ub']:.2f}\")\n",
    "print(f\"Ice: median={ice_thresh['median']:.2f}, q3={ice_thresh['q3']:.2f}, ub={ice_thresh['ub']:.2f}\")\n",
    "\n",
    "\n",
    "sic_for_saving = np.array(sic_for_saving)\n",
    "ice_sheets_for_saving = np.array(ice_sheets_for_saving)\n",
    "\n",
    "print(np.array(sic_for_saving).shape)\n",
    "print(np.array(ice_sheets_for_saving).shape)\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Replace with your actual data\n",
    "# sic_for_graph_building = ...\n",
    "# ice_sheets_for_graph_building = ...\n",
    "\n",
    "# Save the objects to a pickle file\n",
    "with open('sic_and_ice_diffrence_data.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'sic_for_graph_building': sic_for_saving,\n",
    "        'ice_sheets_for_graph_building': ice_sheets_for_saving\n",
    "    }, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main script\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "def plot_paths_graph_unified_directed(all_G, all_paths, thres_def, time_range):\n",
    "    \"\"\"Construct unified directed graph from only the path edges and plot it.\"\"\"\n",
    "\n",
    "    final_G = nx.DiGraph()\n",
    "    final_pos = {}\n",
    "\n",
    "    for G, paths in zip(all_G, all_paths):\n",
    "        if not paths:\n",
    "            print(\"No paths found for this graph, skipping...\")\n",
    "            continue\n",
    "\n",
    "        for path in paths:\n",
    "            for i in range(len(path) - 1):\n",
    "                u, v = path[i], path[i + 1]\n",
    "\n",
    "                # Add nodes and edge (with data if available)\n",
    "                if u not in final_G:\n",
    "                    final_G.add_node(u, **G.nodes[u])\n",
    "                if v not in final_G:\n",
    "                    final_G.add_node(v, **G.nodes[v])\n",
    "\n",
    "                edge_data = G.get_edge_data(u, v, default={})\n",
    "                final_G.add_edge(u, v, **edge_data)\n",
    "\n",
    "                # Position from coordinate in name (e.g., 'O(3,4)')\n",
    "                for node in (u, v):\n",
    "                    if node not in final_pos:\n",
    "                        try:\n",
    "                            y, x = map(int, node[2:-1].split(','))\n",
    "                            final_pos[node] = (x, y)\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "    if not final_G:\n",
    "        print(\"No graph to plot. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Color nodes\n",
    "        # Collect all start nodes from all paths\n",
    "    start_nodes = set(path[0] for paths in all_paths for path in paths if path)\n",
    "\n",
    "    # Color nodes\n",
    "    node_colors = []\n",
    "    for node in final_G.nodes:\n",
    "        label = final_G.nodes[node].get('label', '')\n",
    "        if label == 'O':\n",
    "            if node in start_nodes:\n",
    "                node_colors.append('darkgreen')  # Start node\n",
    "            else:\n",
    "                node_colors.append('lightgreen')  # Other sea ice\n",
    "        elif label == 'L':\n",
    "            node_colors.append('magenta')       # Ice sheet\n",
    "        else:\n",
    "            node_colors.append('gray')          # Unknown\n",
    "\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    ax = plt.gca()\n",
    "    nx.draw(\n",
    "        final_G,\n",
    "        pos=final_pos,\n",
    "        node_color=node_colors,\n",
    "        edge_color='black',\n",
    "        node_size=100,\n",
    "        alpha=0.8,\n",
    "        with_labels=False,\n",
    "        arrows=True,\n",
    "        arrowsize=20,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel('X (Grid Column Index)')\n",
    "    ax.set_ylabel('Y (Grid Row Index)')\n",
    "    ax.set_title(f\"Unified Directed Graph | Time Step {time_range} | {thres_def}\", pad=20)\n",
    "\n",
    "    # Save image\n",
    "    filename = f\"xy_paths_graph_UNIFIED_{thres_def}_{time_range}.png\"\n",
    "    plt.savefig(filename, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved unified graph plot to: {filename}\")\n",
    "\n",
    "    # Save graph to pickle\n",
    "    with open(f\"without_cycle_graph_end_with_3_ice_sheets_{thres_def}_{time_range}.pkl\", 'wb') as f:\n",
    "        pickle.dump({'graph': final_G, 'pos': final_pos}, f)\n",
    "    print(f\"Saved unified graph to: unified_graph_{time_range}.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "#from path_plotting import plot_paths_graph_unified,plot_paths_graph_unified_directed\n",
    "\n",
    "\n",
    "# Threshold definition\n",
    "\n",
    "\n",
    "# Date indices mapping\n",
    "date_indices = {\n",
    "    \"Nov\": 0,      # Nov 1, 2019\n",
    "    \"Dec\": 30,     # Dec 1, 2019\n",
    "    \"Jan\": 61,     # Jan 1, 2020\n",
    "    \"Feb\": 92      # Feb 1, 2020\n",
    "}\n",
    "\n",
    "# Month-wise difference index ranges (start, end)\n",
    "month_ranges = {\n",
    "    #\"Nov\": (0, 29),    # 30 days\n",
    "    \"Dec\": (30, 60),   # 31 days\n",
    "    #\"Jan\": (61, 91),   # 31 days\n",
    "    #\"Feb\": (92, 120)   # 29 days (leap year)\n",
    "}\n",
    "\n",
    "chunk_size = 7  # Number of days per chunk\n",
    "\n",
    "for month_name, (start_idx, end_idx) in month_ranges.items():\n",
    "    print(f\"\\nProcessing {month_name}...\")\n",
    "\n",
    "    all_diff_indices = list(range(start_idx, end_idx + 1))\n",
    "    total_diffs = len(all_diff_indices)\n",
    "\n",
    "    for chunk_start in range(0, total_diffs, chunk_size):\n",
    "        chunk_end = min(chunk_start + chunk_size - 1, total_diffs - 1)\n",
    "        chunk_indices = all_diff_indices[chunk_start:chunk_end + 1]\n",
    "        time_range = (chunk_indices[0], chunk_indices[-1])\n",
    "\n",
    "        print(f\"  - Chunk: Days {chunk_start + 1}-{chunk_end + 1} → Indices {time_range}\")\n",
    "\n",
    "        all_G = []\n",
    "        for time_idx in range(time_range[0], time_range[1] + 1):\n",
    "            # Preprocess data\n",
    "            sic_processed, ice_processed = preprocess_data(\n",
    "                sic_for_graph_building,\n",
    "                ice_sheets_for_graph_building,\n",
    "                time_idx\n",
    "            )\n",
    "\n",
    "            G = analyze_ice_relationships(\n",
    "                sic_processed,\n",
    "                ice_processed,\n",
    "                time_idx,\n",
    "                sic_thresh,\n",
    "                ice_thresh\n",
    "            )\n",
    "\n",
    "            all_G.append(G)\n",
    "\n",
    "        # # Find statistically significant positive paths\n",
    "        sig_paths = find_positive_paths(\n",
    "            all_G,\n",
    "            sic_thresh,\n",
    "            ice_thresh,\n",
    "            time_range,\n",
    "            max_length=11,\n",
    "            M=500\n",
    "        )\n",
    "\n",
    "\n",
    "        print('now all the paths')\n",
    "\n",
    "     \n",
    "\n",
    "        # Custom label for file suffix\n",
    "        file_suffix = f\"{month_name}_days_{chunk_start + 1}-{chunk_end + 1}\"\n",
    "\n",
    "        # # Call plotting functions\n",
    "        plot_paths_graph_unified_directed(all_G, sig_paths, thres_def, file_suffix)\n",
    "\n",
    " \n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "        #plot_paths_xy_coordinates(all_G,sig_paths,thres_def,file_suffix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
